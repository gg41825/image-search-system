# CLIP ONNX Exporter

This script exports [OpenAI CLIP](https://huggingface.co/openai/clip-vit-base-patch32) models
to ONNX format, and optionally deploys them into a [Triton Inference Server](https://github.com/triton-inference-server/server) `model_repository`.

---

## ðŸ“¦ Requirements

- Python 3.9+
- PyTorch 2.x
- Hugging Face Transformers
- ONNX (>=1.14.1 for opset 14, >=1.15.0 for opset 17)
- (Optional) [onnxruntime](https://github.com/microsoft/onnxruntime) to validate ONNX models


## ðŸš€ Usage

Run the script to export both the vision encoder and text encoder:
```
python scripts/export_clip_to_onnx.py
```

By default, the ONNX models will be saved under:
```
onnx_out/
â”œâ”€â”€ clip_image_encoder.onnx
â””â”€â”€ clip_text_encoder.onnx
```
Export + Deploy to Triton

Use the --deploy flag to automatically move the exported ONNX models into the proper Triton structure:
```
python scripts/export_clip_to_onnx.py --deploy
```

Resulting structure:
```
model_repository/
â”œâ”€â”€ clip_image_encoder/
â”‚   â””â”€â”€ 1/
â”‚       â””â”€â”€ model.onnx
â””â”€â”€ clip_text_encoder/
    â””â”€â”€ 1/
        â””â”€â”€ model.onnx
        ```